---
title: Assignment 04
jupyter: python3
---


Regression Analysis in R

Julio Vargas (Boston University)  
September 21, 2025


```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
df.show(5)
print(df.count())
#pd.set_option("display.max_rows", None)  
#pd.DataFrame(df.columns, columns=["Column Names"])
```

## Missing Value Treatment

Replace the missing values in *Salary* by the median of *Salary* based on the *EMPLOYMENT_TYPE* or *EMPLOYMENT_TYPE_NAME*.  
If both are missing, then replace them with the overall median of *Salary*.

```{python}
from pyspark.sql.functions import col, when, isnan, count
# 1. Replace the missing values in Salary by median of Salary based on the EMPLOYMENT_TYPE or EMPLOYMENT_TYPE_NAME. If both are missing, then replace

from pyspark.sql import Window
from pyspark.sql.functions import col, when, isnan, count, expr, median
from pyspark.sql import functions as F

# Calculate overall median salary
overall_median_salary = df.approxQuantile("SALARY", [0.5], 0.01)[0]

# Calculate median_salary by EMPLOYMENT_TYPE
median_by_employment_type = df.groupBy("EMPLOYMENT_TYPE").agg(
    expr("percentile_approx(SALARY, 0.5)").alias("median_salary_emp_type")
)

# Calculate median salary by EMPLOYMENT_TYPE_NAME
median_by_employment_type_name = df.groupBy("EMPLOYMENT_TYPE_NAME").agg(
    expr("percentile_approx(SALARY, 0.5)").alias("median_salary_emp_type_name")
)

# Join median values back to the original dataframe
df_salary_imputed = df.join(median_by_employment_type, on="EMPLOYMENT_TYPE", how="left").join(median_by_employment_type_name, on="EMPLOYMENT_TYPE_NAME", how="left")
df_salary_imputed.show(5)

# Replace missing SALARY values
df_salary_imputed = df_salary_imputed.withColumn("SALARY", when(col("SALARY").isNull(),
    when(col("median_salary_emp_type").isNotNull(), col("median_salary_emp_type"))
    .when(col("median_salary_emp_type_name").isNotNull(), col("median_salary_emp_type_name"))
    .otherwise(overall_median_salary)
).otherwise(col("SALARY")))


print(df_salary_imputed.count())
```

# Feature Engineering

Feature Engineering is a crucial step in preparing your data for machine learning. In this lab, we will focus on the following tasks:

1. Drop rows with missing values in the target variable and key features.
2. By now you are already familiar with the code and the data. Based on your understanding please choose any 3 (my code output has 10) variables as:
   1. three continuous variables and, `MIN_YEARS_EXPERIENCE` (total 4, use your best judgment!)
   2. two categorical.
   3. Your dependent variable (y) is `SALARY`.

3. Convert categorical variables into numerical representations using StringIndexer and OneHotEncoder.
4. Assemble features into a single vector using VectorAssembler.
5. Split the data into training and testing sets.
6. You can use pipeline to do the above steps in one go.
7. Create a new column `MIN_YEARS_EXPERIENCE_SQ` by squaring the `MIN_YEARS_EXPERIENCE` column.
8. Assemble the polynomial features into a new vector column `features_poly` using VectorAssembler.
9. Show the final structure of the DataFrame with the new features.

```{python}
#| eval: true
#| echo: false
#| fig-align: center

from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, regexp_replace, trim
import ast
from pyspark.sql.types import BooleanType, StringType, IntegerType
from pyspark.sql.types import IntegerType,DoubleType,DecimalType

# Drop rows with NA values in relevant columns
regression_df = df_salary_imputed.dropna(subset=[
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EDUCATION_LEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME",
    "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING"
    ]).select(
    "SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE",
    "EDUCATION_LEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME",
    "DURATION", "IS_INTERNSHIP", "COMPANY_IS_STAFFING", "median_salary_emp_type", "median_salary_emp_type_name")



# Categorical columns
categorical_cols = [
    "EDUCATION_LEVELS_NAME", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME",
    "IS_INTERNSHIP", "COMPANY_IS_STAFFING"
]

regression_df=regression_df.withColumn("IS_INTERNSHIP", col("IS_INTERNSHIP").cast(IntegerType()))
regression_df=regression_df.withColumn("COMPANY_IS_STAFFING", col("COMPANY_IS_STAFFING").cast(IntegerType()))

#convert duration to numeric (ind days)
regression_df = regression_df.withColumn("DURATION", col("DURATION").cast(IntegerType()))

regression_df.show(5, truncate=False)

print(regression_df.count())



#regression_df.show(5, truncate=False)
# regression_df.count()   2243



```

```{python}
regression_df.select("DURATION").schema
```

# Linear Regression Model (OLS)

```{python}
# Clean Education Levels by cleaning \n and array brackets
from pyspark.sql.functions import regexp_replace, trim
regression_df = regression_df.withColumn("EDUCATION_LEVELS_NAME",trim(regexp_replace(col("EDUCATION_LEVELS_NAME"), r"[\[\]\n]", "")))


# Index and One-Hot Encode
indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_idx", handleInvalid='skip') for col in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{col}_idx", outputCol=f"{col}_vec") for col in categorical_cols]


# Assemble base features (for GLR and Random Forest)
assembler = VectorAssembler(
    inputCols=[
        "MIN_YEARS_EXPERIENCE", "DURATION"
    ] + [f"{col}_vec" for col in categorical_cols],
    outputCol="features"
)

pipeline = Pipeline(stages=indexers + encoders + [assembler])
regression_data = pipeline.fit(regression_df).transform(regression_df)

regression_data.show(5, truncate=False)

regression_data.select("SALARY", "features").show(5, truncate=False)
```

# Train/Test Split

- Perform a random split of the data into training and testing sets.
- Set a random seed for reproducibility.
- You can choose a number for splitting to your liking, justify your choice.

```{python}
regression_data.select("DURATION").schema
```

```{python}
#| eval: true
#| echo: false
#| fig-align: center

# Split Data
regression_train, regression_test = regression_data.randomSplit([0.8, 0.2], seed=42)
print((regression_data.count(), len(regression_data.columns)))
print((regression_train.count(), len(regression_train.columns)))
print((regression_test.count(), len(regression_test.columns)))
```

## Linear Regression (Get this model from Lab 5.2)

- Train a **Linear Regression** model using the training data.  
  This model is from Lab 5.2 with three more added features.  
- Make sure to use the **features** column from the assembled DataFrame to fit the model.  
- You will run into an important issue here. Please make an effort in figuring it out by yourself.  
  This is one of the most asked interview questions in CapitalOne's management recruiting program.  
- Evaluate the model on the test data.  
- Print the coefficients, intercept, **R²**, **RMSE**, and **MAE**.  
- Use the **summary** object to extract the coefficients and their standard errors, t-values, and p-values.  
- Create a DataFrame to display the coefficients, standard errors, t-values, p-values, and confidence intervals.  
- Interpret the coefficients and their significance, and explain the model performance metrics.

```{python}
from pyspark.ml.regression import GeneralizedLinearRegression

feature_names = assembler.getInputCols()

# featuresCol="features" → the vector with all your input variables.
# labelCol="SALARY" → the target variable to predict.
# family="gaussian" normal distribution
# maxIter=10 → number of iterations for least squares
# regParam=0.3 → regularization parameter (L2 regularization by default)
glr = GeneralizedLinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    family="gaussian", 
    link="identity", 
    maxIter=10, 
    regParam=0.3 
)
# Train the GLR model using the training dataset to learn the relationship between features and SALARY
glr_model = glr.fit(regression_train)
summary = glr_model.summary
```

```{python}
regression_train.select("DURATION").schema
```

```{python}
# Coefficients and Intercept
print("Intercept: {:.4f}".format(glr_model.intercept))
print("Coefficients:")
for i, coef in enumerate(glr_model.coefficients):
    print(f" Feature {i + 1}: {coef:.4f}")
```

```{python}
# Summary stats
print("\n--- Regression Summary ---")
print("Coefficient Standard Errors:", [f"{val:.4f}" for val in summary.coefficientStandardErrors])
print("T Values:", [f"{val:.4f}" for val in summary.tValues])
print("P Values:", [f"{val:.4f}" for val in summary.pValues])
```

```{python}
# print(f"\nDispersion: {summary.dispersion:.4f}")
print(f"Null Deviance: {summary.nullDeviance:.4f}")
print(f"Residual DF Null: {summary.residualDegreeOfFreedomNull}")
print(f"Deviance: {summary.deviance:.4f}")
print(f"Residual DF: {summary.residualDegreeOfFreedom}")
print(f"AIC: {summary.aic:.4f}")
```

```{python}
# 1. Pull feature names directly from Java backend
feature_names = summary._call_java("featureNames")

# 2. Construct full table including intercept
features = ["Intercept"] + feature_names
coefs = [glr_model.intercept] + list(glr_model.coefficients)
se = list(summary.coefficientStandardErrors)
tvals = list(summary.tValues)
pvals = list(summary.pValues)

#This block ensures all regression output values (coefficients, errors, t-values, p-values) align correctly before building a summary table.
print("---This is Diagnostic check, No need to print it in the final doc---")
print("Length of features:", len(features))
print("Length of coefs:", len(coefs))
print("Length of se:", len(se))
print("Length of tvals:", len(tvals))
print("Length of pvals:", len(pvals))
```

```{python}
import pandas as pd
from tabulate import tabulate
from IPython.display import HTML

coef_table = pd.DataFrame({
    "Feature": features,
    "Estimate": [f"{v:.4f}" if v is not None else None for v in coefs],
    "Std Error": [f"{v:.4f}" if v is not None else None for v in se],
    "t-stat": [f"{v:.4f}" if v is not None else None for v in tvals],
    "p-Value": [f"{v:.4f}" if v is not None else None for v in pvals]
})

# 4. Save for report
coef_table.to_csv("output/glr_summary.csv", index=False)

# 5. Optional pretty print
HTML(coef_table.to_html())
```

```{python}
print(1617.0 - 25155.0, 1617.0 + 25155.0)
```

```{python}
#| eval: true
#| echo: false
#| fig-align: center

# Index and One-Hot Encode
# Create squared term for Polynomial Regression
poly_data = regression_data.withColumn("MAX_YEARS_EXPERIENCE_SQ", pow(col("MAX_YEARS_EXPERIENCE"), 2))

# Assemble polynomial features
assembler_poly = VectorAssembler(
    inputCols=[
        "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE_SQ", "MAX_YEARS_EXPERIENCE",
        "DURATION"
    ] + [f"{col}_vec" for col in categorical_cols],
    outputCol="features_poly"
)

poly_data = assembler_poly.transform(poly_data)

# Show final structure
poly_data.select("SALARY",  "features_poly").show(5, truncate=False) #"SALARY", "features", "features_poly"
```

```{python}
# Split Data
polyreg_train, polyreg_test = poly_data.randomSplit([0.8, 0.2], seed=42)
print((poly_data.count(), len(poly_data.columns)))
print((polyreg_train.count(), len(polyreg_train.columns)))
print((polyreg_test.count(), len(polyreg_test.columns)))
```

```{python}
from pyspark.ml.regression import GeneralizedLinearRegression

feature_names = assembler.getInputCols()

poly_glr_max_years = GeneralizedLinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    family="gaussian", # normal distribution
    link="identity", # standard linear regression
    maxIter=10, # number of iterations for least squares
    regParam=0.3 # regularization parameter (L2 regularization by default)
)

poly_glr_max_years_model = poly_glr_max_years.fit(poly_data)
poly_summary = poly_glr_max_years_model.summary


```

```{python}
# Coefficients and Intercept
print("Intercept: {:.4f}".format(poly_glr_max_years_model.intercept))
print("Coefficients:")
for i, coef in enumerate(poly_glr_max_years_model.coefficients):
    print(f" Feature {i + 1}: {coef:.4f}")

# Summary stats
print("\n--- Regression Summary ---")
print("Coefficient Standard Errors:", [f"{val:.4f}" for val in poly_summary.coefficientStandardErrors])
print("T Values:", [f"{val:.4f}" for val in poly_summary.tValues])
print("P Values:", [f"{val:.4f}" for val in poly_summary.pValues])
```

```{python}
# print(f"\nDispersion: {summary.dispersion:.4f}")
print(f"Null Deviance: {poly_summary.nullDeviance:.4f}")
print(f"Residual DF Null: {poly_summary.residualDegreeOfFreedomNull}")
print(f"Deviance: {poly_summary.deviance:.4f}")
print(f"Residual DF: {poly_summary.residualDegreeOfFreedom}")
print(f"AIC: {poly_summary.aic:.4f}")
```

```{python}
# 1. Pull feature names directly from Java backend
feature_names = poly_summary._call_java("featureNames")

# 2. Construct full table including intercept
poly_features = ["Intercept"] + feature_names
poly_coefs = [poly_glr_max_years_model.intercept] + list(poly_glr_max_years_model.coefficients)
poly_se = list(poly_summary.coefficientStandardErrors)
poly_tvals = list(poly_summary.tValues)
poly_pvals = list(poly_summary.pValues)

print("---This is Diagnostic check, No need to print it in the final doc---")
print("Length of features:", len(poly_features))
print("Length of coefs:", len(poly_coefs))
print("Length of se:", len(poly_se))
print("Length of tvals:", len(poly_tvals))
print("Length of pvals:", len(poly_pvals))
```

```{python}
import pandas as pd
from tabulate import tabulate
from IPython.display import HTML

poly_coef_table = pd.DataFrame({
    "Feature": poly_features,
    "Estimate": [f"{v:.4f}" if v is not None else None for v in poly_coefs],
    "Std Error": [f"{v:.4f}" if v is not None else None for v in poly_se],
    "t-stat": [f"{v:.4f}" if v is not None else None for v in poly_tvals],
    "p-value": [f"{v:.4f}" if v is not None else None for v in poly_pvals]
})

# 4. Save for report
poly_coef_table.to_csv("output/poly_summary.csv", index=False)

# 5. Optional pretty print
HTML(poly_coef_table.to_html(index=False))
```

